{"Readme":{"title":"Readme","links":[],"tags":[],"content":""},"index":{"title":"Hello Hello, I'm Costin üòé","links":["projects","Python","projects/Bachelors"],"tags":[],"content":"       I&#039;m a Software Engineer who graduated from Technical University of Cluj-Napoca (2020-2024). I enjoy challenging myself constantly and I want to dive deep into Machine Learning and AI this year. My expertise currently lies in:  Control Engineering (MATLAB/SIMULINK), I&#039;m somewhat of an intermediate when it comes to robotics (definitely working to improve).   \nI will upload my personal projects and accomplishments through the years using Obsidian.\nYou can find me @:\n\nLinkedIn\nInstagram\nGithub\n\n\n\n                  \n                  Some emphasis on my projects \n                  \n                \n\nThese notes are primarily written with the goal of creating a vault where I can always find my most important projects. Therefore, they are filled with MY information and if you find something that‚Äôs off; please contact me and I will change it right away.\n\n\nWhat I‚Äôve worked on in the past\n\nI‚Äôve been an intern at Bosch for 2 years in a Model-Based team. I have successfully automated the regression test using Python and MATLAB and now I‚Äôm working with SIMULINK mathematical models (we‚Äôre basically simulating cars).\nI was a serious student in Uni (top 10) and I‚Äôve really dedicated much time to learn the concepts that they were teaching me.\nMy bachelors degree is my biggest project yet; having implemented a Kalman Filter in order to achieve sensor fusion between a camera and a IMU in order to get the odometry from an autonomous car.\n\nWhat I‚Äôm currently working on\n\n\n                  \n                  ... \n                  \n                \n\n\nImproving this site by uploading my knowledge and making sure I fill it with every note I have,\nthinking of buying a web camera and implementing visual SLAM on it in order to give orientation of a human for indoor applications,\nLearning C,\nImproving my C++ experience,\nLearning ML,\nLearning AI.\n\n\n"},"projects":{"title":"projects","links":[],"tags":[],"content":""},"projects/Bachelors":{"title":"My Bachelor's Degree Thesis","links":["Visual-Inertial-Odometry-based-on-lane-detection-of-an-autonomous-vehicle-with-constant-speed","Sensor-fusion","Odometry","ego's","BNO055","BLDC-Motor","ESC","Adaptive-Cruise-Control","projects/inertial-odom","projects/Kalman-Filter","projects/visual-odom","projects/Proportional-Derivative","Kinematic-Model","Yaw","Bicycle-Kinematic-Model","projects/Loop-Closure","Pose-Graphs"],"tags":["projects"],"content":"In 2024 I finished my Bachelor‚Äôs studies at Technical University of Cluj-Napoca. My diploma project is intitulated Visual-Inertial Odometry based on lane-detection of an autonomous vehicle with constant speed.\n\nMotivation\n\nI chose this project because of Steven Gong and his experience at F1Tenth; an autonomous racing contest for 1:10 RC Cars.\nAnother inspiration for this project was also SPOT by Boston Dynamics. The main attraction was the Real-Time SLAM algorithm it could execute.\n\nIntroduction\n\n\n                  \n                  What is sensor fusion and odometry? \n                  \n                \n\nSensor fusion is collecting and integrating data from multiple sensors in order to get a better understanding of the process that we‚Äôre monitoring than if it came from a single source.\nOdometry is transforming data coming from sensors that give movement or displacement in order to estimate the ego‚Äôs change in position over time.\n\n\n\n    \n\n\n\n                  \n                  Why constant speed? \n                  \n                \n\n‚ÄúThe linear acceleration signal typically cannot be integrated to recover velocity, or double-integrated to recover position. The error typically becomes larger than the signal within less than 1 second if other sensor sources are not used to compensate this integration error.‚Äù - BNO055 documentation\nnoise = 1[mg] = 0.001[g] = 0.001 \\times 9.81[m/s^{2}] = 0.00981 [m/s^{2}],\ndisplacement = \\frac{1}{2} \\times a \\times \\textbf{t}^{2},\n\\frac{1}{2} \\times a \\times t^{2} \\approx 49[m], \\text{ where } \\textbf{t} = 100[seconds].\nThis right here shows that for 100 [seconds] and a 1 [mg] constant of noise there are 49 [meters] of false displacement. My setup did not allow me to add different movement sensors and so I considered constant speed because the BLDC Motor was in a close loop with the ESC, assuring a Adaptive Cruise Control behaviour.\n\n\nI will further touch on the connections between the components that I used:\n\n    \n\nHere, the NUCLEO took care of the real-time algorithms:\n\ncontrol of the servomotor and the BLDC Motor,\nInertial Odometry\nKalman Filter,\n\nand the Raspberry Pi 4B executed all of the perception algorithms:\n\nLane-Detection\nVisual Odometry\n\nControl of the servomotor in closed-loop\nThe main goal of this was to position the ego on the middle of the lane. As error; it would always get the lateral error in [cm] to the middle line generated by the lane-detection algorithm. The proposed control loop was Proportional-Derivative as it gave the best results when testing.\nKinematic Model\nIn order to get displacement; I needed a Kinematic Model. It was very simplified; as in decomposing the speed vector in the X and Y axis by using the cosinus and sinus trigonometrical functions. The Yaw was updated through a common technique found in the Bicycle Kinematic Model. I can use these formulas because of the constant speed.\n\\Delta{x} = v \\cdot t \\cdot \\cos(\\psi),\n\\Delta{y} = v \\cdot t \\cdot \\sin(\\psi),\nconst = \\frac{360 \\cdot \\Delta{t}}{2 \\cdot \\pi \\cdot L}\n\\psi = \\psi + const \\cdot \\tan(\\theta)\nKalman Filter\n\n\n                  \n                  It performs the fusion of the two sensors, resulting in visual-inertial odometry \n                  \n                \n\nSince the Kalman Filter considers linear processes and this process is clearly nonlinear (trigonometric functions), I can say that I have performed a point-by-point linearization so that the value is considered constant for the entire duration of 100 [ms].\nSince this application only approached the Yaw Rate fusion, this Kalman Filter is of 1st order!\n\n\nI think these next two pictures can perfectly explain the way I implemented the Kalman Filter:\n\n    \n\n\n    \n\nThe formula used for the Kalman Gain (for 1st order) is:\nK = \\frac{\\sigma_{predictie}}{\\sigma_{predictie} + \\sigma_{update}}\nIt tells which phase should get more credibility - the prediction or the update.\nThe results I achieved\n\n\n                  \n                  In order to test the final algorithm I had three circuits at my disposal. One offered to me by the team that organizes the contest in which this car is actually used - Bosch Future Mobility Challenge, one with the shape of letter &quot;S&quot; and one with a circular shape. \n                  \n                \n\nThis being covered, here are my results:\n1st\n\n    \n\n\n    \n\n\n\n                  \n                  Here I wanted to test the ability of the camera to see two 90 degree&#039; corners one after another. The threshold I used was 7 in order to amplify the corners and as you can see the straight line portion was sacrificed. \n                  \n                \n\n2nd\n\n    \n\n\n    \n\n\n\n                  \n                  Normally, the IMU&#039;s performances should decrease in time and here I was pleasantly surprised to see that it&#039;s absolute orientation property could hold on even on longer tracks. Also, the camera held on pretty well in recreating the initial shape of the circuit. \n                  \n                \n\n3rd\n\n    \n\n\n\n                  \n                  These are probably the most satisfying results I succeeded to collect. Sure, the distance was fairly smaller in comparison with the ones above but I would call these results pretty close to being 1:1. In all these 3 cases the Kalman Gain (K) was ‚âà 0.5 so the final result should of course be the mean. \n                  \n                \n\nAlso, I want to point out that the camera is a sensor that is heavily influenced by environmental factors such as lightning or it‚Äôs angle, etc. So when I can control these variables I can get the most out of it.\n\n\n4th\n\n    \n\n\n\n                  \n                  In this particular case I wanted to implement a Loop Closure algorithm based on Pose Graphs. Sadly I did not have time to finish it but it would be the first thing I would try to do if I were to start this project again. Maybe I will but now I really want to approach other topics.\n                  \n                \n\nPossible issues and bad results\nI want to dedicate a small section of this topic to the things that could go wrong with a visual-inertial approach. Firstly, I‚Äôve already mentioned that the camera is a sensor that is highly sensitive to environmental factors such as lightning and angle. In the pictures below, in the first you can see what bad lightning does to the image and how bad it can affect it‚Äôs results in the second one. So; if the initial estimation of the camera is bad then it will ruin the whole run, no matter if the next ones are good.\nTo my surprise, the BNO055 absolute orientation sensor did not fail me at all but then again, I did not have runs longer than 5 minutes. Of course, with time it is destined to suffer drift as well. This is where the loop closure would benefit both downsides.\n\n    \n\n\n    \n\nWhat would follow?\nI would try to implement a SLAM algorithm and I would definitely start with the loop-closure algorithm."},"projects/Gaussian-distribution":{"title":"Gaussian Distribution","links":[],"tags":["projects"],"content":"A Gaussian or normal distribution always has a bell-shaped curve and is determined by two parameters: the mean \\mu and the standard deviation \\sigma. The graph associated with this phenomenon is symmetric with its center at the mean, and its shape is determined by the standard deviation. The value of this function at a point x is given by formula\np(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right) .\nThe term \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} ensures that the integral of the function over the entire range of x converges to 1.\n\\int p(x) \\, dx = 1\n\n    \n"},"projects/Kalman-Filter":{"title":"Kalman Filter","links":["stochastic-mathematics","Markov-property","probability-density-functions","projects/Gaussian-distribution"],"tags":["projects"],"content":"The concept of probabilistic or stochastic mathematics deals with the study and modeling of uncertainties. It seeks to combine concepts from probability theory with statistical methods to understand and make decisions under conditions of uncertainty. Stochastic processes represent probabilistic models for phenomena that evolve in space or time. For example, Markov chains model systems where the future depends only on the present state and not on the distant past; a concept known as the Markov property.\nTo better understand this concept I followed Sebastian Thrun‚Äôs infamous book Probabilistic Robotics.\nYou can find the paper here: docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf\nThe mathematics behind\nLet X be a variable and x an event. If the domain of values that X can take is discrete (such as the outcome of a coin toss - heads or tails), then it is written as:\n p(X = x) \nFrom here on, I will use the notation p(x). As the processes are continuous, the random variables can take on a range of values, and it is considered that all these variables have probability density functions (PDF). An example of such a function is the first-order Gaussian distribution with mean ùúá and standard deviation ùúé:\np(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right)\nIn case of multiple variables, this PDF looks like this:\np(x) = \\det(2 \\pi \\Sigma)^{- \\frac{1}{2}} \\exp \\left( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right)\n\n\n                  \n                  The Kalman filter is an implementation of the Bayesian filter. This filter is Gaussian and is designed for continuous linear systems. The a priori phase of the Kalman filter is Gaussian if the following properties hold: \n                  \n                \n\n\nThe state p(x_t \\; | \\; u_t,x_{t-1}) has the following linear form:\n\nx_t = A_t x_{t-1} + B_t u_t + \\epsilon_t\n\n\nHere, x_t‚Äã and x_{t-1} represent the state vectors of the system. A_t‚Äã is an n \\times n matrix where n is the number of states of the system, and B_t is an n \\times m matrix where m is the number of control inputs.\n\n\nThe variable \\epsilon_t‚Äã represents a random vector that accounts for the uncertainties of reality (white Gaussian noise with zero mean). This implies that the errors are uncorrelated over time. The covariance of \\epsilon_t‚Äã is denoted by R_t.\n\n\n\nThe measurement probability p(z_t \\; | \\; x_t), like the state of the system, must also have a linear form with added white noise:\n\nz_t = C_t x_t + \\delta_t\n\nC_t‚Äã is a k \\times n matrix, where k is the dimension of the measurement vector. The vector \\delta_t also represents white Gaussian noise with zero mean, and its covariance is denoted by Q_t‚Äã.\n\n\nThe assumption bel(x_0) must be normally distributed.\n\nKalman Filter Algorithm\nInput: \n\nPrevious state ( \\mu_{t-1} ) \nPrevious covariance ( \\Sigma_{t-1} ) \nCurrent control inputs ( u_t ) \nCurrent measurements ( z_t ) \n\nOutput: \n\nCurrent estimated state ( \\mu_t ) \nCurrent covariance ( \\Sigma_t ) \n\nSteps:\n\n\nPrediction of the current state:\n\\bar{\\mu_t} = A_t \\mu_{t-1} + B_t u_t\n\n\nPrediction of the covariance matrix:\n\\bar{\\Sigma_t} = A_t \\Sigma_{t-1} A_t^\\top + R_t\n\n\nCalculation of the Kalman Gain:\nK_t = \\bar{\\Sigma_t} C_t^\\top (C_t \\bar{\\Sigma_t} C_t^\\top + Q_t)^{-1}\n\n\nState update:\n\\mu_t = \\bar{\\mu_t} + K_t (z_t - C_t \\bar{\\mu_t})\n\n\nUpdate of the covariance matrix:\n\\Sigma_t = (I - K_t C_t) \\bar{\\Sigma_t}\n\n\n\n\n                  \n                  Based on six plots, the functionality of the Kalman filter can be represented as follows: \n                  \n                \n\na) The initial assumption bex(x_0) with its characteristic bell shape.\nb) The measurement is taken with added white noise.\nc) The prediction phase.\nd) The new assumption after applying a control input.\ne) A new assumption based on the new measurement with added white noise.\nf) The update phase.\n\n\n\n    \n"},"projects/Loop-Closure":{"title":"Loop Closure","links":[],"tags":["projects"],"content":"\n    \n"},"projects/Proportional-Derivative":{"title":"Proportional-Derivative Control","links":["control","Derivative","Pass-Down-Filter"],"tags":["projects"],"content":"This control was mainly used in my bachelor‚Äôs degree project in order to adjust the angle of the servomotor so that the ego was situating itself in the middle of the road.\n \n    \n\nThe proposed feedback loop is in the following picture:\n \n    \n\nThe Derivative term needed a Pass-Down Filter in order to make it implementable on a microcontroller because the derivative suggests the future (improper system). Of course, the pass-down filter adds a -20 [dB\\dec] slope where the ‚Äús‚Äù term introduces a +20 [dB\\dec] slope and as such getting a 0 [dB\\dec] slope at the end.\n \n    \n"},"projects/inertial-odom":{"title":"Inertial Odometry","links":["BNO055"],"tags":["projects"],"content":"Inertial Odometry is great because it does not care of the environment variables. I used a BNO055 which is an ‚Äúabsolute orientation‚Äù sensor made by Bosch. The main disadvantage of this type of sensor is that in time it‚Äôs readings can only decrease. The variable I wanted to extract from this sensor was the rotation along the Z-axis which is also called ‚ÄúYaw Rate‚Äù and it is given by the gyroscope.\n\\dot{\\psi} = \\omega_z,\n\\psi(t) = \\psi(0) + \\int_{0}^{\\textbf{t}} \\dot{\\psi}(\\tau) \\, d\\tau."},"projects/visual-odom":{"title":"Visual Odometry","links":["projects/Bachelors","RANSAC","inliers","outliers","algorithm"],"tags":["projects"],"content":"Visual Odometry was my main contribution to my bachelor‚Äôs degree. It represents the update step in the Kalman Filter and I based it on the lane-detection algorithm that was already implemented on the Raspberry. It‚Äôs goal is to give the yaw-rate with respect to a starting point.\nThe main disadvantage of this approach is that the camera is very sensitive to any environmental variables, the most significant one being the proper lightning.\n\n\n                  \n                  I approached this case by using the RANSAC - Random Sample Consensus algorithm. \n                  \n                \n\nBy taking the POV to Bird-Eye-View, I succeeded in constraining the lines to always be parallel to each other. Having this achieved, I only needed to find out how to determine the rotation from one frame to the next one.\n\n\n\n    \n\nTo enhance the confidence of the rotation I used RANSAC because the camera measurements can be very noisy depending on the lightning, angle, etc. It succeeds in separating the data into inliers and outliers and it tries to fit a model based on the inliers.\n\n    \n\nThe algorithm used is based on affine transformations which preserves the parallelism of the lanes by solving the next equational system:\n\\begin{aligned}\n    T(x) &amp;= Ax + b, &amp; \\quad A &amp;= \\begin{bmatrix}\n        a &amp; b \\\\\n        c &amp; d\n    \\end{bmatrix},\n\\end{aligned}\n\\begin{bmatrix}\nx_i&#039; \\\\\ny_i&#039;\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na &amp; b \\\\\nc &amp; d\n\\end{bmatrix}\n\\begin{bmatrix}\nx_i \\\\\ny_i\n\\end{bmatrix}\n+\n\\begin{bmatrix}\nt_x \\\\\nt_y\n\\end{bmatrix},\n\\begin{bmatrix}\nx_1 &amp; y_1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; x_1 &amp; y_1 &amp; 1 \\\\\n\\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\\nx_n &amp; y_n &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 0 &amp; x_n &amp; y_n &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\na \\\\\nb \\\\\nt_x \\\\\nc \\\\\nd \\\\\nt_y\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1&#039; \\\\\ny_1&#039; \\\\\n\\vdots \\\\\nx_n&#039; \\\\\ny_n&#039;\n\\end{bmatrix},\n\\begin{aligned}\n    scale = \\sqrt{a^2+b^2}, &amp; \\quad R = \\begin{bmatrix}\n\\cos \\theta &amp; -\\sin \\theta \\\\\n\\sin \\theta &amp; \\cos \\theta\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{a}{\\text{scale}} &amp; \\frac{b}{\\text{scale}} \\\\\n\\frac{c}{\\text{scale}} &amp; \\frac{d}{\\text{scale}}\n\\end{bmatrix}\n\\end{aligned}\nThe third equation is only the second one but extended. In my case I has a vector of pixel coordinates for X and Y.\n\n\n                  \n                  How good were the results? \n                  \n                \n\nI would say it depends on more factors but I won‚Äôt go in depth. The only one I am willing to share is that I came up with a threshold in order to determine if the curve is to the left; to the right; or simply a straight line.\nBy using a threshold of 7, I managed to accentuate the angle of the curves, but the straight path suffered in this representation. On the other hand, a treshold of 10 accentuated the straight path while the curves had a little bit more to suffer.\n\n\n  Threshold of 10    Threshold of 7   "}}